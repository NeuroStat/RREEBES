---
title: 'Reproduce analyses of Ward et al (2014): Complexity is costly'
author: "Aurélie Garnier, Frank Pennekamp, Mikael Pontarp"
date: "1 Jun 2015"
output:
  html_document:
    toc: yes
  pdf_document:
    toc: yes
---

#Introduction

This report is the reproduction of the analyses shown in [Ward, E.J., Holmes, E.E., Thorson, J.T. & Collen, B. (2014) Complexity is costly: A meta-analysis of parametric and non-parametric methods for short-term population forecasting. Oikos, 123, 652–661](http://onlinelibrary.wiley.com/doi/10.1111/j.1600-0706.2014.00916.x/abstract).

Eric Ward kindly provided a subset of the raw data, processed data as well as the R scripts used for model fitting; they can be found in the respective folders. His original repository can be found [here](https://github.com/eric-ward/complexity-costly). Because the data originates from a collaborative project, not all data is freely available yet; however, the data provided by Eric allows to reproduce the analyses on the fish time-series shown in the paper.

# Data processing 

```{r, setup, eval=t, echo=F, warning=F}
rm(list=ls())

# run package installations (may require additional packages depending on your R setup)
#library(devtools)
#devtools::install_github("James-Thorson/NLTS")

# libraries need for prediction
#install.packages(c("MARSS", "NTLS", "mgcv", "np", "locfit", "forecast", "kernlab", "tsDyn", "ltsa", "timsac", "randomForest"))

# libraries for data management
library(tidyr)
library(plyr)
library(lubridate)
library(stringr)
library(ggplot2)
library(RCurl)
library(reshape2)

# libraries used for forecasting population dynamics
library(tsDyn)
library(MARSS)
library(mgcv)
library(np)
library(locfit)
library(forecast)
library(kernlab)
library(ltsa)
library(timsac)
library(randomForest)
library(NLTS)
```

Read time series data provided from repository:

```{r}
ts <- read.csv(text=getURL("https://raw.githubusercontent.com/opetchey/RREEBES/WARD_etal_2014_Oikos/WARD_etal_2014_Oikos/processed%20data/masterDat%20052015.csv"), header=T, stringsAsFactors=F)
metainfo <- read.csv(text=getURL("https://raw.githubusercontent.com/opetchey/RREEBES/WARD_etal_2014_Oikos/WARD_etal_2014_Oikos/processed%20data/Data%20and%20metadata%20052015.csv"), header=T, stringsAsFactors=F)

#look at data structure
str(ts)
head(ts)

# number of fish ts
length(unique(ts$ID))

ts$ID_old <- as.factor(ts$ID)
ts$ID_new <- as.numeric(ts$ID_old)
ts$ID <- ts$ID_new

```

Explore some time series visually to get an idea of the variability. Figure does not appear in the paper. Time series were centered to easily plot them simultaneousl [e.g. Year-Mean(Year) and Value - Mean(Value)].

```{r, warning=FALSE, fig.width=10}
set.seed(12345678)
ggplot(data=subset(ts, ID %in% sample(ts$ID,size=15)), aes(x=Year-mean(Year), y=Value-mean(Value))) + geom_line() + facet_wrap(ID~Species,ncol=5,nrow=3, scales="free")
```

# Results

## Time series forecasting

The forecasting script was provided but required some adjusting to run on the available data. It is not shown here for its length. Running the script will produce forecasts based on the training data (all data points except the five last data points). This script takes considerable time to run as it loops over each time series and fits the 49 different model parameterizations. An object containing the output of the full run can be found here: https://www.dropbox.com/s/p1j16tgzlrwrr7m/Ward_et_al_results.RData?dl=0

MARSS failed on a couple of time series (IDs: 430, 514, 515, 590, 1234) which were hence excluded.

```{r, eval=F, echo=F}
# Load in the data. This dataset only contains the 
set.seed(12345678)

#dat <- subset(ts, ID %in% sample(ts$ID,size=15))
# dat <- subset(ts, Database %in% c("RAMlegacy.catch","RAMlegacy.ssb","RAMlegacy.recperssb"))
#dat <- subset(ts, Database %in% c("salmon"))
dat <- ts[!(ts$ID %in% c(430,514,515,590,1234)),]
#dat <- ts

NAHEAD = 5 # number of points ahead to forecast
MODELS = 50  # number of candidates (this is overestimate)

# dimension predicted and predictedSE to have the same dimensions
# as dat, so that each observation will have a predicted value and se
predicted = matrix(NA,dim(dat)[1],MODELS) 
predictedSE = matrix(NA,dim(dat)[1],MODELS)

numRec = max(unique(dat$ID)) # number of unique time series 
model.output = array(list(),c(numRec,MODELS)) # list of model objects

# source Jim's file. Note that a more recent version
# of this function is here, https://github.com/James-Thorson/NLTS
# SourceURL <- "https://raw.githubusercontent.com/opetchey/RREEBES/WARD_etal_2014_Oikos/WARD_etal_2014_Oikos/code/Fn_simplex_and_smap_2012-05-04.r"
# source_url(SourceURL)

for(w in setdiff(1:numRec, c(430,514,515,590,1234))){
  
  print(w)
  
  y = dat$Value[which(dat$ID==w)]
  
  # to standardize by the mean, uncomment
  #y = (y-mean(y,na.rm=T))/sd(y,na.rm=T)
  n.y = length(y) # length of time series
  y.train = y[1:(n.y-5)]
  y.test = y[(n.y-5+1):n.y]
  x = seq(1,length(y.train))
  
  model.count = 1
  model.names = rep("",MODELS)
  
  # Generalized additive model / GAM with spline over time/years
  gamfit = gam(y.train~s(x))
  # predict function for gam requires a new list of predictor variables as newdata
  pred = predict(gamfit, newdata = list("x" = seq(1,n.y)), se.fit=TRUE)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = pred$fit[(n.y-NAHEAD+1):n.y]
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = pred$se.fit[(n.y-NAHEAD+1):n.y]
  model.output[[w,model.count]] = gamfit
  model.names[model.count] = "GAM (gam)"  
  model.count = model.count + 1

  # Neural network time series model with embedding dimension = 1, hidden units (size) = 1
  nnetTs.11 = nnetTs(ts(y.train),m=1,d=1,steps=1,size=1)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = predict(nnetTs.11,n.ahead = NAHEAD)
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = NA
  model.output[[w,model.count]] = nnetTs.11
  model.names[model.count] = "nnetTs.11 (nnetTs)"  
  model.count = model.count + 1

  # Neural network time series model with embedding dimension = 1, hidden units (size) = 2
  nnetTs.12 = nnetTs(y.train,m=1,size=2)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = predict(nnetTs.12,n.ahead = NAHEAD)
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = NA
  model.output[[w,model.count]] = nnetTs.12
  model.names[model.count] = "nnetTs.12 (nnetTs)"  
  model.count = model.count + 1

  # Neural network time series model with embedding dimension = 2, hidden units (size) = 1  
  nnetTs.21 = nnetTs(y.train,m=2,size=1)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = predict(nnetTs.21,n.ahead = NAHEAD)
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = NA
  model.output[[w,model.count]] = nnetTs.21
  model.names[model.count] = "nnetTs.21 (nnetTs)"  
  model.count = model.count + 1
  
  # Neural network time series model with embedding dimension = 2, hidden units (size) = 2  
  nnetTs.22 = nnetTs(y.train,m=2,size=2)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = predict(nnetTs.22,n.ahead = NAHEAD)
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = NA
  model.output[[w,model.count]] = nnetTs.22
  model.names[model.count] = "nnetTs.22 (nnetTs)"  
  model.count = model.count + 1 

  # Neural network time series model with embedding dimension = 3, hidden units (size) = 1
  nnetTs.31 = nnetTs(y.train,m=3,size=1)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = predict(nnetTs.31,n.ahead = NAHEAD)
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = NA
  model.output[[w,model.count]] = nnetTs.31
  model.names[model.count] = "nnetTs.31 (nnetTs)"  
  model.count = model.count + 1 

  # Neural network time series model with embedding dimension = 3, hidden units (size) = 2  
  nnetTs.32 = nnetTs(y.train,m=3,size=2)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = predict(nnetTs.32,n.ahead = NAHEAD)
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = NA
  model.output[[w,model.count]] = nnetTs.32
  model.names[model.count] = "nnetTs.32 (nnetTs)"  
  model.count = model.count + 1
  
  # Simple random walk with no drift using the rwf() function
  fit.nodrift <- rwf(y.train,h=NAHEAD) # ~ random walk forecast model, no drift
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = fit.nodrift$mean
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = (fit.nodrift$upper[,2] - fit.nodrift$mean)/1.96
  model.output[[w,model.count]] = fit.nodrift
  model.names[model.count] = "AR - no drift (rwf)"  
  model.count = model.count + 1    

  # Simple random walk with drift using the rwf() function  
  fit.drift <- rwf(y.train,h=NAHEAD, drift=TRUE) # ~ random walk forecast model, with drift
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = fit.drift$mean
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = (fit.drift$upper[,2] - fit.drift$mean)/1.96
  model.output[[w,model.count]] = fit.drift
  model.names[model.count] = "AR - drift (rwf)"  
  model.count = model.count + 1   
  
  # Step-wise fractionally differenced auto-arima fitted model
  autoarima <- arfima(y.train) 
  f = forecast(autoarima,NAHEAD)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = f$mean
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = (f$upper[,2] - f$mean)/1.96 
  model.output[[w,model.count]] = autoarima
  model.names[model.count] = "ARIMA - with frac diff (arfima)"  
  model.count = model.count + 1

  # Exponentially smoothed time series forecasts. Frequency here is just included a priori, not estimated
  fit.es <- ets(ts(y.train,frequency=1)) # exponentially smoothed time series forecasts
  f = forecast(fit.es,NAHEAD)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = f$mean
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = (f$upper[,2] - f$mean)/1.96 
  model.output[[w,model.count]] = fit.es
  model.names[model.count] = "Exp smooth (freq=1) (ets)"  
  model.count = model.count + 1

  # Exponentially smoothed time series forecasts. Frequency here is just included a priori, not estimated.
  # Higher order frequencies included to try to capture age structure for things like aggregate salmon counts.    
  fit.es <- ets(ts(y.train,frequency=2)) # exponentially smoothed time series forecasts
  f = forecast(fit.es,NAHEAD)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = f$mean
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = (f$upper[,2] - f$mean)/1.96 
  model.output[[w,model.count]] = fit.es
  model.names[model.count] = "Exp smooth (freq=2) (ets)"  
  model.count = model.count + 1
  
  # Exponentially smoothed time series forecasts. Frequency here is just included a priori, not estimated.
  # Higher order frequencies included to try to capture age structure for things like aggregate salmon counts.        
  fit.es <- ets(ts(y.train,frequency=3)) # exponentially smoothed time series forecasts
  f = forecast(fit.es,NAHEAD)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = f$mean
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = (f$upper[,2] - f$mean)/1.96 
  model.output[[w,model.count]] = fit.es
  model.names[model.count] = "Exp smooth (freq=3) (ets)"  
  model.count = model.count + 1

  # Exponentially smoothed time series forecasts. Frequency here is just included a priori, not estimated.
  # Higher order frequencies included to try to capture age structure for things like aggregate salmon counts.      
  fit.es <- ets(ts(y.train,frequency=4)) # exponentially smoothed time series forecasts
  f = forecast(fit.es,NAHEAD)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = f$mean
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = (f$upper[,2] - f$mean)/1.96 
  model.output[[w,model.count]] = fit.es
  model.names[model.count] = "Exp smooth (freq=4) (ets)"  
  model.count = model.count + 1
    
  # Structural time series model  
  fit.sts <- StructTS(y.train) # structural time series
  f = forecast(fit.sts,NAHEAD)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = f$mean
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = (f$upper[,2] - f$mean)/1.96  
  model.output[[w,model.count]] = fit.sts
  model.names[model.count] = "Structural time series (freq=1) (StructTS)"  
  model.count = model.count + 1
    
  # Structural time series model. 
  # Higher order frequencies included to try to capture age structure for things like aggregate salmon counts.     
  fit.sts <- StructTS(ts(y.train,frequency=2)) # structural time series
  f = forecast(fit.sts,NAHEAD)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = f$mean
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = (f$upper[,2] - f$mean)/1.96  
  model.output[[w,model.count]] = fit.sts
  model.names[model.count] = "Structural time series (freq=2) (StructTS)"  
  model.count = model.count + 1
    
  # Explore multiple options for locally weighted regression. Try linear, quadratic, cubic.
  # Also explore dimension of nearest neighbors and bandwidth, etc.
  # These loops are terribly inefficient, but find the best solution
  mse.best = 100
  pars = 0
  # these loops are over the nearest neighbors, bandwidth, and polynomial
  for(i in seq(0.2,3,0.05)) {
      # 1st order
      locreg = locfit(y.train~lp(x,nn=i, deg = 1))
      #mse.prop = sum((y[(n.y-NAHEAD+1):n.y]-predict(locreg, newdata = list("x" = seq(1,n.y)))[(n.y-NAHEAD+1):n.y])^2)
      mse.prop = mean((y.train-fitted(locreg))^2)
      if(mse.prop < mse.best) {
        pars = c(i, 1)
        mse.best = mse.prop       
      }

      # 2nd order
      locreg = locfit(y.train~lp(x,nn=i, deg = 2))
      mse.prop = mean((y.train-fitted(locreg))^2)
      if(mse.prop < mse.best) {
        pars = c(i, 2)
        mse.best = mse.prop	
      }   

      # 3rd order
      locreg = locfit(y.train~lp(x,nn=i, deg = 3))
      mse.prop = mean((y.train-fitted(locreg))^2)
      if(mse.prop < mse.best) {
        pars = c(i, 3)
        mse.best = mse.prop	
      }   
       
  } # end i
  
  # This local regression uses the best parameters found above for nearest neighbors (nn)
  # and degree of polynomial (deg)
  locreg = locfit(y.train~lp(x, nn=pars[1], deg = pars[2])) 
  f = predict(locreg, newdata = list("x" = seq(1,n.y)))[(n.y-NAHEAD+1):n.y]
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = f
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = NA
  model.output[[w,model.count]] = locreg
  model.names[model.count] = "Local regression (locfit)"  
  model.count = model.count + 1
    
  # non-parameteric automatic bandwith selection (fixed, adaptive nn, generalized nn)
  model.np <- npreg(y.train ~ x, regtype = "ll", bwmethod = "cv.aic",gradients = TRUE)
  f = predict(model.np, newdata = list("x" = seq(1,n.y)))[(n.y-NAHEAD+1):n.y]
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = f
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = NA
  model.output[[w,model.count]] = model.np
  model.names[model.count] = "Non-param bandwidth (npreg)"  
  model.count = model.count + 1
    
  # ARIMA models. We abandoned using auto.arima and wanted to look at whether certain orders were supported. So 
  # we are fitting each model manually
  auto.arima = NULL
  try(auto.arima <- arima(ts(y.train,frequency=1), order = c(1,0,1)),silent=T)
  if(is.null(auto.arima)==F) {
  f = forecast(auto.arima,NAHEAD)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = f$mean
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = (f$upper[,2] - f$mean)/1.96  
  model.output[[w,model.count]] = auto.arima
  }
  model.names[model.count] = "ARIMA 1.0.1 (arima)"  
  model.count = model.count + 1 

  # ARIMA models. We abandoned using auto.arima and wanted to look at whether certain orders were supported. So 
  # we are fitting each model manually
  auto.arima = NULL
  try(auto.arima <- arima(ts(y.train,frequency=1), order = c(1,0,0)),silent=T)
  if(is.null(auto.arima)==F) {
  f = forecast(auto.arima,NAHEAD)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = f$mean
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = (f$upper[,2] - f$mean)/1.96  
  model.output[[w,model.count]] = auto.arima
  }
  model.names[model.count] = "ARIMA 1.0.0 (arima)"  
  model.count = model.count + 1 
    
  # ARIMA models. We abandoned using auto.arima and wanted to look at whether certain orders were supported. So 
  # we are fitting each model manually
  auto.arima = NULL
  try(auto.arima <- arima(ts(y.train,frequency=1), order = c(2,0,1)),silent=T)
  if(is.null(auto.arima)==F) {
  f = forecast(auto.arima,NAHEAD)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = f$mean
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = (f$upper[,2] - f$mean)/1.96  
  model.output[[w,model.count]] = auto.arima
  }
  model.names[model.count] = "ARIMA 2.0.1 (arima)"  
  model.count = model.count + 1  

  # ARIMA models. We abandoned using auto.arima and wanted to look at whether certain orders were supported. So 
  # we are fitting each model manually
  auto.arima = NULL
  try(auto.arima <- arima(ts(y.train,frequency=1), order = c(1,0,2)),silent=T)
  if(is.null(auto.arima)==F) {
  f = forecast(auto.arima,NAHEAD)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = f$mean
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = (f$upper[,2] - f$mean)/1.96  
  model.output[[w,model.count]] = auto.arima
  }
  model.names[model.count] = "ARIMA 1.0.2 (arima)"  
  model.count = model.count + 1   

  # ARIMA models. We abandoned using auto.arima and wanted to look at whether certain orders were supported. So 
  # we are fitting each model manually
  auto.arima = NULL  
  try(auto.arima <- arima(ts(y.train,frequency=1), order = c(2,0,2)),silent=T)
  if(is.null(auto.arima)==F) {
  f = forecast(auto.arima,NAHEAD)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = f$mean
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = (f$upper[,2] - f$mean)/1.96  
  model.output[[w,model.count]] = auto.arima
  }
  model.names[model.count] = "ARIMA 2.0.2 (arima)"  
  model.count = model.count + 1

  # ARIMA models. We abandoned using auto.arima and wanted to look at whether certain orders were supported. So 
  # we are fitting each model manually
  auto.arima = NULL  
  try(auto.arima <- arima(ts(y.train,frequency=1), order = c(0,0,1)),silent=T)
  if(is.null(auto.arima)==F) {
  f = forecast(auto.arima,NAHEAD)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = f$mean
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = (f$upper[,2] - f$mean)/1.96  
  model.output[[w,model.count]] = auto.arima
  }
  model.names[model.count] = "ARIMA 0.0.1 (arima)"  
  model.count = model.count + 1
 
  # ARIMA models. We abandoned using auto.arima and wanted to look at whether certain orders were supported. So 
  # we are fitting each model manually
  auto.arima = NULL  
  try(auto.arima <- arima(ts(y.train,frequency=1), order = c(0,0,2)),silent=T)
  if(is.null(auto.arima)==F) {
  f = forecast(auto.arima,NAHEAD)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = f$mean
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = (f$upper[,2] - f$mean)/1.96  
  model.output[[w,model.count]] = auto.arima
  }
  model.names[model.count] = "ARIMA 0.0.2 (arima)"  
  model.count = model.count + 1 

  # ARIMA models. We abandoned using auto.arima and wanted to look at whether certain orders were supported. So 
  # we are fitting each model manually
  auto.arima = NULL  
  try(auto.arima <- arima(ts(y.train,frequency=1), order = c(2,0,0)),silent=T)
  if(is.null(auto.arima)==F) {
  f = forecast(auto.arima,NAHEAD)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = f$mean
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = (f$upper[,2] - f$mean)/1.96  
  model.output[[w,model.count]] = auto.arima
  }
  model.names[model.count] = "ARIMA 2.0.0 (arima)"  
  model.count = model.count + 1  
      
  # ARIMA models. We abandoned using auto.arima and wanted to look at whether certain orders were supported. So 
  # we are fitting each model manually
  auto.arima = NULL
  try(auto.arima <- arima(ts(y.train,frequency=1), order = c(1,1,1)),silent=T)
  if(is.null(auto.arima)==F) {
  f = forecast(auto.arima,NAHEAD)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = f$mean
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = (f$upper[,2] - f$mean)/1.96  
  model.output[[w,model.count]] = auto.arima
  }
  model.names[model.count] = "ARIMA 1.1.1 (arima)"  
  model.count = model.count + 1 
  
  # ARIMA models. We abandoned using auto.arima and wanted to look at whether certain orders were supported. So 
  # we are fitting each model manually
  auto.arima = NULL
  try(auto.arima <- arima(ts(y.train,frequency=1), order = c(1,1,0)),silent=T)
  if(is.null(auto.arima)==F) {
  f = forecast(auto.arima,NAHEAD)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = f$mean
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = (f$upper[,2] - f$mean)/1.96  
  model.output[[w,model.count]] = auto.arima
  }
  model.names[model.count] = "ARIMA 1.1.0 (arima)"  
  model.count = model.count + 1
    
  # ARIMA models. We abandoned using auto.arima and wanted to look at whether certain orders were supported. So 
  # we are fitting each model manually
  auto.arima = NULL
  try(auto.arima <- arima(ts(y.train,frequency=1), order = c(2,1,1)),silent=T)
  if(is.null(auto.arima)==F) {
  f = forecast(auto.arima,NAHEAD)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = f$mean
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = (f$upper[,2] - f$mean)/1.96  
  model.output[[w,model.count]] = auto.arima
  }
  model.names[model.count] = "ARIMA 2.1.1 (arima)"  
  model.count = model.count + 1  

  # ARIMA models. We abandoned using auto.arima and wanted to look at whether certain orders were supported. So 
  # we are fitting each model manually
  auto.arima = NULL
  try(auto.arima <- arima(ts(y.train,frequency=1), order = c(1,1,2)),silent=T)
  if(is.null(auto.arima)==F) {
  f = forecast(auto.arima,NAHEAD)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = f$mean
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = (f$upper[,2] - f$mean)/1.96  
  model.output[[w,model.count]] = auto.arima
  }
  model.names[model.count] = "ARIMA 1.1.2 (arima)"  
  model.count = model.count + 1   

  # ARIMA models. We abandoned using auto.arima and wanted to look at whether certain orders were supported. So 
  # we are fitting each model manually
  auto.arima = NULL  
  try(auto.arima <- arima(ts(y.train,frequency=1), order = c(2,1,2)),silent=T)
  if(is.null(auto.arima)==F) {
  f = forecast(auto.arima,NAHEAD)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = f$mean
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = (f$upper[,2] - f$mean)/1.96  
  model.output[[w,model.count]] = auto.arima
  }
  model.names[model.count] = "ARIMA 2.1.2 (arima)"  
  model.count = model.count + 1

  # ARIMA models. We abandoned using auto.arima and wanted to look at whether certain orders were supported. So 
  # we are fitting each model manually
  auto.arima = NULL  
  try(auto.arima <- arima(ts(y.train,frequency=1), order = c(0,1,1)),silent=T)
  if(is.null(auto.arima)==F) {
  f = forecast(auto.arima,NAHEAD)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = f$mean
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = (f$upper[,2] - f$mean)/1.96  
  model.output[[w,model.count]] = auto.arima
  }
  model.names[model.count] = "ARIMA 0.1.1 (arima)"  
  model.count = model.count + 1
 
  # ARIMA models. We abandoned using auto.arima and wanted to look at whether certain orders were supported. So 
  # we are fitting each model manually
  auto.arima = NULL  
  try(auto.arima <- arima(ts(y.train,frequency=1), order = c(0,1,2)),silent=T)
  if(is.null(auto.arima)==F) {
  f = forecast(auto.arima,NAHEAD)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = f$mean
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = (f$upper[,2] - f$mean)/1.96  
  model.output[[w,model.count]] = auto.arima
  }
  model.names[model.count] = "ARIMA 0.1.2 (arima)"  
  model.count = model.count + 1 

  # ARIMA models. We abandoned using auto.arima and wanted to look at whether certain orders were supported. So 
  # we are fitting each model manually
  auto.arima = NULL  
  try(auto.arima <- arima(ts(y.train,frequency=1), order = c(2,1,0)),silent=T)
  if(is.null(auto.arima)==F) {
  f = forecast(auto.arima,NAHEAD)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = f$mean
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = (f$upper[,2] - f$mean)/1.96  
  model.output[[w,model.count]] = auto.arima
  }
  model.names[model.count] = "ARIMA 2.1.0 (arima)"  
  model.count = model.count + 1  
  
  # gaussian process model. Like the time series approaches above, we can explore the effects
  # of treating these as having different frequencies to try to capture age structure in things
  # like salmon
  filter <- gausspr(ts(y.train,frequency=1)~x,kernel="rbfdot",kpar="automatic",type="regression")
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = predict(filter, newdata = list("x" = seq(1,n.y)))[(n.y-NAHEAD+1):n.y]
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = NA
  model.output[[w,model.count]] = filter
  model.names[model.count] = "Gaussian process (freq=1) (gausspr)"  
  model.count = model.count + 1  

  # gaussian process model. Like the time series approaches above, we can explore the effects
  # of treating these as having different frequencies to try to capture age structure in things
  # like salmon  
  filter <- gausspr(ts(y.train,frequency=2)~x,kernel="rbfdot",kpar="automatic",type="regression")
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = predict(filter, newdata = list("x" = seq(1,n.y)))[(n.y-NAHEAD+1):n.y]
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = NA
  model.output[[w,model.count]] = filter
  model.names[model.count] = "Gaussian process (freq=2) (gausspr)"  
  model.count = model.count + 1

  # gaussian process model. Like the time series approaches above, we can explore the effects
  # of treating these as having different frequencies to try to capture age structure in things
  # like salmon  
  filter <- gausspr(ts(y.train,frequency=3)~x,kernel="rbfdot",kpar="automatic",type="regression")
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = predict(filter, newdata = list("x" = seq(1,n.y)))[(n.y-NAHEAD+1):n.y]
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = NA
  model.output[[w,model.count]] = filter
  model.names[model.count] = "Gaussian process (freq=3) (gausspr)"  
  model.count = model.count + 1

  # gaussian process model. Like the time series approaches above, we can explore the effects
  # of treating these as having different frequencies to try to capture age structure in things
  # like salmon  
  filter <- gausspr(ts(y.train,frequency=4)~x,kernel="rbfdot",kpar="automatic",type="regression")
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = predict(filter, newdata = list("x" = seq(1,n.y)))[(n.y-NAHEAD+1):n.y]
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = NA
  model.output[[w,model.count]] = filter
  model.names[model.count] = "Gaussian process (freq=4) (gausspr)"  
  model.count = model.count + 1

  # state space time series model, with drift. No temporal autocorrelation in errors
  mod = NULL
  try(mod <- MARSS(y.train,method="BFGS",silent=TRUE))
  if(is.null(mod)==F) {
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = mod$states[1,length(y.train)]+seq(1,5)*mod$par$U
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = sqrt(mod$par$Q*seq(1,5))
  model.output[[w,model.count]] = mod
  model.names[model.count] = "MARSSdrift"  
  model.count = model.count + 1 
 }
 
 # state space time series model, with no drift. No temporal autocorrelation in errors
  mod = NULL
  try(mod <- MARSS(y.train,method="BFGS",model=list(U = matrix(0,1,1)),silent=TRUE))
  if(is.null(mod)==F) {
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = mod$states[1,length(y.train)]
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = sqrt(mod$par$Q*seq(1,5)) 
  model.output[[w,model.count]] = mod
  model.names[model.count] = "MARSSnodrift"  
  model.count = model.count + 1  
  }
  
  # use the Simplex and procedure from sugihara et al. here. Implemented in Jim Thorson's function
  simplex.embed=EmbedFn(y.train, PredInterval=1, Candidates=1:7)
  simplex.oneAhead = NltsPred(c(y.train,NA), PredInterval=1, Nembed=simplex.embed, PredNum=c(length(y.train)+1), Method="Simplex")
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):(n.y-NAHEAD+1)],model.count] = simplex.oneAhead
  predicted[which(dat$ID==w)[(n.y-NAHEAD+2):(n.y-NAHEAD+2)],model.count] = NA
  predicted[which(dat$ID==w)[(n.y-NAHEAD+3):(n.y-NAHEAD+3)],model.count] = NA
  predicted[which(dat$ID==w)[(n.y-NAHEAD+4):(n.y-NAHEAD+4)],model.count] = NA
  predicted[which(dat$ID==w)[(n.y-NAHEAD+5):(n.y-NAHEAD+5)],model.count] = NA          
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = NA
  model.output[[w,model.count]] = list(simplex.embed, simplex.oneAhead)
  model.names[model.count] = "Simplex v1"  
  model.count = model.count + 1  
    
  y = dat$Value[which(dat$ID==w)]
  n.y = length(y) # length of time series
  y.train = y[1:(n.y-5)]
  y.test = y[(n.y-5+1):n.y]
  x = seq(1,length(y.train))
  
  # use the Simplex and procedure from sugihara et al. here. Implemented in Jim Thorson's function. This 
  # differs from the simplex above in that it's predicting 1:4 time steps ahead
  simplex.embed = EmbedFn(c(y.train,NA), PredInterval=1, Candidates=1:7)
  simplex.oneAhead = NltsPred(c(y.train,NA), PredInterval=1, Nembed=simplex.embed, PredNum=c(length(y.train)+1), Method="Simplex")  # PredInterval is the number of years in next year, etc)
  simplex.embed = EmbedFn(c(y.train,NA,NA), PredInterval=2, Candidates=1:7)
  simplex.twoAhead = NltsPred(c(y.train,NA,NA), PredInterval=2, Nembed=simplex.embed, PredNum=c(length(y.train)+2), Method="Simplex")  # PredInterval is the number of years in next year, etc)  
  simplex.embed = EmbedFn(c(y.train,NA,NA,NA), PredInterval=3, Candidates=1:7)
  simplex.threeAhead = NltsPred(c(y.train,NA,NA,NA), PredInterval=3, Nembed=simplex.embed, PredNum=c(length(y.train)+3), Method="Simplex")  # PredInterval is the number of years in next year, etc)
  simplex.embed = EmbedFn(c(y.train,NA,NA,NA,NA), PredInterval=4, Candidates=1:7)
  simplex.fourAhead = NltsPred(c(y.train,NA,NA,NA,NA), PredInterval=4, Nembed=simplex.embed, PredNum=c(length(y.train)+4), Method="Simplex")  # PredInterval is the number of years in next year, etc)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):(n.y-NAHEAD+1)],model.count] = simplex.oneAhead
  predicted[which(dat$ID==w)[(n.y-NAHEAD+2):(n.y-NAHEAD+2)],model.count] = simplex.twoAhead
  predicted[which(dat$ID==w)[(n.y-NAHEAD+3):(n.y-NAHEAD+3)],model.count] = simplex.threeAhead
  predicted[which(dat$ID==w)[(n.y-NAHEAD+4):(n.y-NAHEAD+4)],model.count] = simplex.fourAhead
  predicted[which(dat$ID==w)[(n.y-NAHEAD+5):(n.y-NAHEAD+5)],model.count] = NA          
  model.output[[w,model.count]] = list(simplex.embed, simplex.oneAhead)
  model.names[model.count] = "Simplex v2"  
  model.count = model.count + 1
    
  # Sugihara's S-map procedure, as implemented in Jim Thorson's function. This is just 1-step ahead
  smap.embed = EmbedFn(y.train, PredInterval=1, Candidates=1:7)
  smap.theta = ThetaFn(y.train, PredInterval=1, Nembed=smap.embed)$Max
  smap.oneAhead = NltsPred(c(y.train,NA), Nembed=smap.embed, PredNum=length(y.train)+1, PredInterval=1, Method="Smap", Theta=smap.theta)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):(n.y-NAHEAD+1)],model.count] = smap.oneAhead
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):(n.y-NAHEAD+1)],model.count] = NA
  model.output[[w,model.count]] = list("Embed" = smap.embed, "Theta"=smap.theta)
  model.names[model.count] = "SMAP v1"  
  model.count = model.count + 1  

  # Sugihara's S-map procedure, as implemented in Jim Thorson's function. This is as above but does time steps 1:4 steps ahead
  y = dat$Value[which(dat$ID==w)]
  n.y = length(y) # length of time series
  y.train = y[1:(n.y-5)]
  y.test = y[(n.y-5+1):n.y]
  x = seq(1,length(y.train))

  smap.embed = EmbedFn(c(y.train,NA), PredInterval=1, Candidates=1:7)
  theta = ThetaFn(c(y.train,NA), PredInterval=1, Nembed=smap.embed)$Max
  smap.oneAhead = NltsPred(c(y.train,NA), PredInterval=1, Nembed=smap.embed, PredNum=c(length(y.train)+1), Method="Smap", Theta=theta)
  smap.embed = EmbedFn(c(y.train,NA,NA), PredInterval=2, Candidates=1:7)
  theta = ThetaFn(c(y.train,NA,NA), PredInterval=2, Nembed=smap.embed)$Max
  smap.twoAhead = NltsPred(c(y.train,NA,NA), PredInterval=2, Nembed=smap.embed, PredNum=c(length(y.train)+2), Method="Smap", Theta=theta)
  smap.embed = EmbedFn(c(y.train,NA,NA,NA), PredInterval=3, Candidates=1:7)
  theta = ThetaFn(c(y.train,NA,NA,NA), PredInterval=3, Nembed=smap.embed)$Max
  smap.threeAhead = NltsPred(c(y.train,NA,NA,NA), PredInterval=3, Nembed=smap.embed, PredNum=c(length(y.train)+3), Method="Smap", Theta=theta)
  smap.embed = EmbedFn(c(y.train,NA,NA,NA,NA), PredInterval=4, Candidates=1:7)
  theta = ThetaFn(c(y.train,NA,NA,NA,NA), PredInterval=4, Nembed=smap.embed)$Max
  smap.fourAhead = NltsPred(c(y.train,NA,NA,NA,NA), PredInterval=4, Nembed=smap.embed, PredNum=c(length(y.train)+4), Method="Smap", Theta=theta)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):(n.y-NAHEAD+1)],model.count] = smap.oneAhead
  predicted[which(dat$ID==w)[(n.y-NAHEAD+2):(n.y-NAHEAD+2)],model.count] = smap.twoAhead
  predicted[which(dat$ID==w)[(n.y-NAHEAD+3):(n.y-NAHEAD+3)],model.count] = smap.threeAhead
  predicted[which(dat$ID==w)[(n.y-NAHEAD+4):(n.y-NAHEAD+4)],model.count] = smap.fourAhead
  predicted[which(dat$ID==w)[(n.y-NAHEAD+5):(n.y-NAHEAD+5)],model.count] = NA          
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = NA
  model.output[[w,model.count]] = list(smap.embed, smap.oneAhead)
  model.names[model.count] = "SMAP v2"
  model.count = model.count + 1

  # Random forest approach to time series model w/lags. This block just does 1-step ahead forecasts
  y = dat$Value[which(dat$ID==w)]
  n.y = length(y) # length of time series
  y.train = y[1:(n.y-5)]  
  nX = length(y.train)
  ranfor <- randomForest(y=y.train[6:nX], x=cbind(y.train[1:(nX-5)],y.train[2:(nX-4)],y.train[3:(nX-3)],y.train[4:(nX-2)],y.train[5:(nX-1)]))
  
# only do predictions one step ahead
  ranfor.pred = predict(ranfor, y[(nX-4):(nX)])
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):(n.y-NAHEAD+1)],model.count] = ranfor.pred
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):(n.y-NAHEAD+1)],model.count] = NA  
  model.output[[w,model.count]] = ranfor
  model.names[model.count] = "randomForest v1"  
  model.count = model.count + 1 
  
  n.y = length(y) # length of time series
  y.train = y[1:(n.y-5)]
  y.test = y[(n.y-5+1):n.y]
  x = seq(1,length(y.train))

  nX = length(y.train)
  ranfor <- randomForest(y=y.train[6:nX], x=cbind(y.train[1:(nX-5)],y.train[2:(nX-4)],y.train[3:(nX-3)],y.train[4:(nX-2)],y.train[5:(nX-1)]))
  # 1 step ahead
  ranfor.pred = predict(ranfor, y[(nX-4):(nX)])
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):(n.y-NAHEAD+1)],model.count] = ranfor.pred
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):(n.y-NAHEAD+1)],model.count] = NA  
  
  # 2 steps ahead
  ranfor <- randomForest(y=y.train[7:nX], x=cbind(y.train[1:(nX-6)],y.train[2:(nX-5)],y.train[3:(nX-4)],y.train[4:(nX-3)],y.train[5:(nX-2)]))
  ranfor.pred = predict(ranfor, y[(nX-4):(nX)])
  predicted[which(dat$ID==w)[(n.y-NAHEAD+2):(n.y-NAHEAD+2)],model.count] = ranfor.pred
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+2):(n.y-NAHEAD+2)],model.count] = NA
  
  # 3 steps ahead
  ranfor <- randomForest(y=y.train[8:nX], x=cbind(y.train[1:(nX-7)],y.train[2:(nX-6)],y.train[3:(nX-5)],y.train[4:(nX-4)],y.train[5:(nX-3)]))
  ranfor.pred = predict(ranfor, y[(nX-4):(nX)])
  predicted[which(dat$ID==w)[(n.y-NAHEAD+3):(n.y-NAHEAD+3)],model.count] = ranfor.pred
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+3):(n.y-NAHEAD+3)],model.count] = NA
    
  # 4 steps ahead
  ranfor <- randomForest(y=y.train[9:nX], x=cbind(y.train[1:(nX-8)],y.train[2:(nX-7)],y.train[3:(nX-6)],y.train[4:(nX-5)],y.train[5:(nX-4)]))
  ranfor.pred = predict(ranfor, y[(nX-4):(nX)])
  predicted[which(dat$ID==w)[(n.y-NAHEAD+4):(n.y-NAHEAD+4)],model.count] = ranfor.pred
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+4):(n.y-NAHEAD+4)],model.count] = NA    
    
  model.output[[w,model.count]] = ranfor
  predicted[which(dat$ID==w)[(n.y-NAHEAD+5):(n.y-NAHEAD+5)],model.count] = NA          
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = NA
  model.output[[w,model.count]] = list(smap.embed, smap.oneAhead)
  model.names[model.count] = "randomForest v2"  
  model.count = model.count + 1
  
  # Simple linear regression model
  n.y = length(y) # length of time series
  y.train = y[1:(n.y-5)]
  y.test = y[(n.y-5+1):n.y]
  x = seq(1,length(y.train))

  # Simple regression approach using lm() - observation error only
  lmfit = lm(y.train~x)
  # predict function for gam requires a new list of predictor variables as newdata
  #predicted[[2]] = predict(gamfit, newdata = list("x" = seq(1,n.y)))[(n.y-NAHEAD+1):n.y]
  pred = predict(lmfit, newdata = list("x" = seq(1,n.y)), se.fit=TRUE)
  predicted[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = pred$fit[(n.y-NAHEAD+1):n.y]
  #predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = pred$se.fit[(n.y-NAHEAD+1):n.y]
  predictedSE[which(dat$ID==w)[(n.y-NAHEAD+1):n.y],model.count] = pred$residual.scale
  model.output[[w,model.count]] = lmfit  
  model.names[model.count] = "simple regression"    
  
}

predicted <- as.data.frame(predicted)
names(predicted) <- model.names
dat <- cbind(dat,predicted)

prediction <- gather(dat, model_type, Value_predicted,  11:(MODELS+10))
```

## Calculation of mean absolute scaled error

Implementation of the absolute scaled error (ASE) for a single time-series. The absolute error is scaled by the mean absolute
error within the training data to account for the different variability per time series. ASE is calculated for the different forecasting methods.

````{r ASE_function, eval=F}
get.ASE <- function(x){
  # x is the data used in ddply for each "ID_new" and "model_type"
  #x <- subset(prediction, ID_new==1 & model_type == "GAM (gam)")
  training_dd <- x[1:(nrow(x)-5),]
  mean.abs.error <-( 1/(nrow(training_dd)-1) ) * sum(abs(training_dd[2:nrow(training_dd),"Value"] - training_dd[1:nrow(training_dd)-1,"Value"]))
  
  res_ASE <- rep(NA,nrow(x)) 
  step_ahead <- rep(NA,nrow(x))
  for (t in 1:5){
    res_ASE[nrow(training_dd)+t] <- abs(x[nrow(training_dd)+t,"Value"]-x[nrow(training_dd)+t,"Value_predicted"]) / mean.abs.error
    step_ahead[nrow(training_dd)+t] <- t
  }
  
  #res_ASE[(nrow(x)-4):nrow(x)] <- sapply(1:5, function(t){abs(x[nrow(training_dd)+t,"Value"]-x[nrow(training_dd)+t,"Value_predicted"]) / mean.abs.error})
    
  cbind.data.frame(x,ASE=res_ASE, step_ahead=step_ahead)
}

#out put with ASE estimate
prediction_ASE <- ddply(.data=prediction, c("ID_new","model_type"), get.ASE)
#save.image("/Volumes/Work MAC backup/Ward_et_al_results.RData")
```

# Figures

Prepare data for visualization by re-ordering factor levels. For some of the models it is not entirely clear which of the different specifications was taken.
```{r, echo=F}
# load object with script output
load("/Users/Frank/Documents/Github projects/RREEBES/WARD_etal_2014_Oikos/report/Ward_et_al_results.RData")

models_selected <- subset(prediction_ASE, model_type %in% c("GAM (gam)", "nnetTs.11 (nnetTs)", "AR - no drift (rwf)", "Local regression (locfit)", "ARIMA 1.0.1 (arima)","Exp smooth (freq=1) (ets)" , "ARIMA 0.0.1 (arima)", "ARIMA 1.1.0 (arima)", "MARSSdrift", "SMAP v2", "Simplex v2" , "randomForest v2", "simple regression"))
models_selected$model_type <- factor(models_selected$model_type)
models_selected$model_type <- factor(models_selected$model_type,levels(models_selected$model_type)[c(13,7,3,6,4,8,9,1,5,2,11,10,12)])
models_selected$data_dist <- ifelse(models_selected$model_type %in% c("GAM (gam)", "nnetTs.11 (nnetTs)", "AR - no drift (rwf)", "Local regression (locfit)", "ARIMA 1.0.1 (arima)","Exp smooth (freq=1) (ets)" , "ARIMA 0.0.1 (arima)", "ARIMA 1.1.0 (arima)", "MARSSdrift", "simple regression"), "parametric", "non-parametric")

```

## Reproducing Figure 1

Visualize overall forecasting performance using the 13 models specified in the caption. The plot in the manuscript shows only 12 models! Currently it is hard to determine the exact grouping used (e.g. salmon, marine fish productivity) because there is no corresponding label shipped with the data. Nevertheless, the plots pretty much look like Figure 1 from the paper!

```{r, fig.width=15, fig.height=10}
mean_ASE <- ddply(models_selected, .(Database, model_type, data_dist, step_ahead), summarize, mean_log_ASE = log(mean(ASE, na.rm = T)))
       
ggplot(data=subset(mean_ASE, step_ahead<5 & mean_log_ASE<60 & (Database == "salmon" | Database == "RAMlegacy.recperssb")), aes(x=model_type,y=mean_log_ASE, group=step_ahead, linetype=as.factor(step_ahead), size=as.factor(step_ahead), shape=data_dist)) + geom_point(size=2) + geom_line() + 
  theme(axis.text.x  = element_text(angle=90, vjust=0.5, size=16)) + scale_linetype_manual(values=c("solid", "dashed", "dotted", "solid")) + 
  scale_size_manual(values=c(0.8,0.8,0.8,0.4))  + scale_shape_manual(values=c(2,1)) + facet_wrap(~Database)

# mean_ASE_salmon_all <- ddply(subset(models_selected, Species %in% c("Chinook", "Chum", "Pink", "Coho", "Sockeye")), .(model_type, step_ahead), summarize, mean_log_ASE = log(mean(ASE, na.rm = T)))
# 
# ggplot(data=subset(mean_ASE_salmon_all, step_ahead<5), aes(x=model_type,y=mean_log_ASE, group=as.factor(step_ahead), linetype=as.factor(step_ahead), size=as.factor(step_ahead))) + geom_point() + geom_line() + theme(axis.text.x  = element_text(angle=90, vjust=0.5, size=16)) + scale_size_manual(values=c(0.8,0.8,0.8,0.4)) + scale_linetype_manual(values=c("solid", "dashed", "dotted", "solid")) 

```

## Reproducing Figure 2

Plot MASE for the different species of salmon separately. No "Coho" in the current dataset, hence no plot produced. Nevertheless, plots pretty similar to what was shown in the paper.

```{r, fig.width=15, fig.height=12}
mean_ASE_salmon <- ddply(subset(models_selected, Species %in% c("Chinook", "Chum", "Pink", "Coho", "Sockeye")), .(Species, model_type, data_dist, step_ahead), summarize, mean_log_ASE = log(mean(ASE, na.rm = T)))
mean_ASE_salmon$Species <- factor(mean_ASE_salmon$Species ,levels(as.factor(mean_ASE_salmon$Species))[c(3,2,4,5,1)])

ggplot(data=subset(mean_ASE_salmon, step_ahead<5), aes(x=model_type,y=mean_log_ASE, group=as.factor(step_ahead), linetype=as.factor(step_ahead), size=as.factor(step_ahead), shape=data_dist)) + geom_point(size=2) + geom_point() + geom_line() + theme(axis.text.x  = element_text(angle=90, vjust=0.5, size=16)) + facet_wrap(~Species) + scale_linetype_manual(values=c("solid", "dashed", "dotted", "solid")) + scale_size_manual(values=c(0.8,0.8,0.8,0.4)) + scale_shape_manual(values=c(2,1))

```

## No reproduction of Figure 3 

```{r, echo=F, eval=F}
# only load dplyr package after model fitting due to clashes with tsDyn package
library(dplyr)

# rename IDs in meta information dataset to match data available

# number of fish ts
metainfo$ID_old<- as.factor(metainfo$ID)
metainfo$ID_new <- as.numeric(metainfo$ID_old)
metainfo$ID <- metainfo$ID_new

# the absolute scaled error (ASE) statistic from the GAM model, averaged over forecasts of 1 to 3 time steps
mean_ASE_GAM <- prediction_ASE %>% 
  filter(model_type == "GAM (gam)" & step_ahead %in% 1:3) %>%
  group_by(ID_old) %>%
  summarize(mean_ASE = mean(ASE))

covariates <- unique(metainfo[, c(1:3, 8:24)])

pred_covar <- merge(mean_ASE_GAM, covariates, by=c("ID_old"))

fish_age <- pred_covar %>% filter(Database %in% c("RAMlegacy.recperssb"))

#The expected improvement in ASE is calculated as the ASE statistic divided by the ASE statistic at the mean of each covariate (e.g. mean trophic level of 2.5 for birds), 100 * ASEt / ASEmean(x).

lm(log(mean_ASE) ~ maxAge, data=fish_age)

ggplot(data=fish_age[complete.cases(fish_age$maxAge),], aes(x=maxAge, y=log(mean_ASE))) + stat_smooth(method="lm") + geom_point() + geom_vline(xintercept=mean(fish_age$maxAge, na.rm=T))

length(fish_age[!is.na(fish_age$maxAge)==TRUE, ]$maxAge)

```

## Partial reproduction of Figure 4

Based on meta information data set. This is pretty approximate, but ok as it was not the main target of the reproduction.
```{r, fig.width=15, fig.height=10}
ggplot(subset(metainfo, Database %in% c("salmon" , "RAMlegacy.recperssb")), aes(x=training.acf)) + geom_histogram() + facet_wrap(~Database) + geom_vline(xintercept = 0, linetype="dashed", size=2)
```





